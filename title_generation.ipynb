{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bert_score\n\nimport csv\nimport logging\nimport random\n\nimport torch\nimport transformers\nfrom bert_score import score\nfrom transformers import GPT2DoubleHeadsModel, GPT2Tokenizer\n\ntransformers.tokenization_utils.logger.setLevel(logging.ERROR)\ntransformers.configuration_utils.logger.setLevel(logging.ERROR)\ntransformers.modeling_utils.logger.setLevel(logging.ERROR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Cuda available:\", torch.cuda.is_available())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_local_model(model):\n    print(\"Loading\", model)\n    model = GPT2DoubleHeadsModel.from_pretrained(\"models/\" + model + \"/\", local_files_only=True)\n    model.to(\"cuda\")\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_tokenizer():\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    special_tokens = {'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<pad>',\n                      'additional_special_tokens': ['<TITLE>']}\n    tokenizer.add_special_tokens(special_tokens)\n    return tokenizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_abstracts_for_qual_eval():\n       abstracts = [{\n        \"Title\": \"GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples\",\n        \"Abstract\": \"Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks. However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples. In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected. One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi- Supervised Generative Adversarial Networks. In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting. Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks.\"},\n        {\"Title\": \"Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition\",\n         \"Abstract\": \"Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. However, modeling a realistic user simulator is challenging. A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents. Two agents interact with each other and are jointly learned simultaneously. The method uses the actor-critic framework to facilitate pretraining and improve scalability. We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction.\"},\n        {\"Title\": \"Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge\",\n         \"Abstract\": \"Chinese relation extraction is conducted using neural networks with either character-based or word-based inputs, and most existing methods typically suffer from segmentation errors and ambiguity of polysemy. To address the issues, we propose a multi-grained lattice framework (MG lattice) for Chinese relation extraction to take advantage of multi-grained language information and external linguistic knowledge. In this framework, (1) we incorporate word-level information into character sequence inputs so that segmentation errors can be avoided. (2) We also model multiple senses of polysemous words with the help of external linguistic knowledge, so as to alleviate polysemy ambiguity. Experiments on three real-world datasets in distinct domains show consistent and significant superiority and robustness of our model, as compared with other baselines. We will release the source code of this paper in the future.\"},\n        {\"Title\": \"Empirically Estimating Order Constraints for Content Planning in Generation\",\n         \"Abstract\": \"In a language generation system, a content planner embodies one or more \\u201cplans\\u201d that are usually hand\\u2013crafted, sometimes through manual analysis of target text. In this paper, we present a system that we developed to automatically learn elements of a plan and the ordering constraints among them. As training data, we use semantically annotated transcripts of domain experts performing the task our system is designed to mimic. Given the large degree of variation in the spoken language of the transcripts, we developed a novel algorithm to find parallels between transcripts based on techniques used in computational genomics. Our proposed methodology was evaluated two\\u2013fold: the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89%. A qualitative evaluation is also provided.\"},\n        {\"Title\": \"Improving Distant Supervision for Information Extraction Using Label Propagation Through Lists\",\n         \"Abstract\": \"Because of polysemy, distant labeling for information extraction leads to noisy training data. We describe a procedure for reducing this noise by using label propagation on a graph in which the nodes are entity mentions, and mentions are coupled when they occur in coordinate list structures. We show that this labeling approach leads to good performance even when off-the-shelf classifiers are used on the distantly-labeled data.\"},\n        {\"Title\": \"An Extension of BLANC to System Mentions\",\n         \"Abstract\": \"BLANC is a link-based coreference evaluation metric for measuring the quality of coreference systems on gold mentions. This paper extends the original BLANC (\\u201cBLANC-gold\\u201d henceforth) to system mentions, removing the gold mention assumption. The proposed BLANC falls back seamlessly to the original one if system mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data.\"},\n        {\"Title\": \"Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\",\n         \"Abstract\": \"Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.\"},\n        {\"Title\": \"Compositional Questions Do Not Necessitate Multi-hop Reasoning\",\n         \"Abstract\": \"Multi-hop reading comprehension (RC) questions are challenging because they require reading and reasoning over multiple paragraphs. We argue that it can be difficult to construct large multi-hop RC datasets. For example, even highly compositional questions can be answered with a single hop if they target specific entity types, or the facts needed to answer them are redundant. Our analysis is centered on HotpotQA, where we show that single-hop reasoning can solve much more of the dataset than previously thought. We introduce a single-hop BERT-based RC model that achieves 67 F1-comparable to state-of-the-art multi-hop models. We also design an evaluation setting where humans are not shown all of the necessary paragraphs for the intended multi-hop reasoning but can still answer over 80% of questions. Together with detailed error analysis, these results suggest there should be an increasing focus on the role of evidence in multi-hop reasoning and possibly even a shift towards information retrieval style evaluations with large and diverse evidence collections.\"},\n        {\"Title\": \"Multi-Domain Dialogue Acts and Response Co-Generation\",\n         \"Abstract\": \"Generating fluent and informative responses is of critical importance for task-oriented dialogue systems. Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. There are at least two shortcomings with such approaches. First, the inherent structures of multi-domain dialogue acts are neglected. Second, the semantic associations between acts and responses are not taken into account for response generation. To address these issues, we propose a neural co-generation model that generates dialogue acts and responses concurrently. Unlike those pipeline approaches, our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed. We train the two modules jointly using an uncertainty loss to adjust their task weights adaptively. Extensive experiments are conducted on the large-scale MultiWOZ dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations.\"},\n        {\"Title\": \"Structured Tuning for Semantic Role Labeling\",\n         \"Abstract\": \"Recent neural network-driven semantic role labeling (SRL) systems have shown impressive improvements in F1 scores. These improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear SRL models. Introducing the benefits of structure to inform neural models presents a methodological challenge. In this paper, we present a structured tuning framework to improve models using softened constraints only at training time. Our framework leverages the expressiveness of neural networks and provides supervision with structured loss components. We start with a strong baseline (RoBERTa) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints. Additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios.\"},\n    ]\n\n    return abstracts","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def import_abstracts(count=-1):\n    pairs = []\n    with open(\"abstracts.csv\", newline='', encoding='UTF-8') as testfile:\n        reader = csv.DictReader(testfile)\n        for row in reader:\n            pairs.append({\"Title\": row[\"title\"], \"Abstract\": row[\"abstract\"]})\n\n    if count == -1:\n        return pairs\n    return random.sample(pairs, count)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def import_test_abstracts(model_name, count=-1):\n    abstracts = []\n    with open(f\"models/{model_name}/test.txt\", \"r\") as file:\n        data = file.read()\n        inputs_idx = data.index(\"inputs\")\n        inputs = data[inputs_idx + 10:len(data) - 18]\n        sequences = inputs.split('<|endoftext|> \", ')\n        for seq in sequences:\n            s = seq.split(\" <TITLE> \")\n            abstracts.append({\"Abstract\": s[0][1:], \"Title\": s[1]})\n\n        if count == -1:\n            return abstracts\n        return random.sample(abstracts, count)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_titles_from_abstracts(model, tokenizer, abstracts, \n                              num_return_sequences=5, max_length=25, min_length=5,\n                              diversity_penalty=1.0, length_penalty=10.0, repetition_penalty=2.0):\n    titles = []\n    for i, abstract in enumerate(abstracts):\n        if (i + 1) % 100 == 0:\n            print(f\"{i + 1}/{len(abstracts)}\")\n        text = abstract[\"Abstract\"] + \" <TITLE> \"\n        inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda\")\n\n        title_ids = model.generate(inputs,\n                                   num_beams=num_return_sequences,\n                                   num_beam_groups=num_return_sequences,\n                                   num_return_sequences=num_return_sequences,\n                                   max_length=len(inputs[0]) + max_length,\n                                   min_length=len(inputs[0]) + min_length,\n                                   early_stopping=True,\n                                   diversity_penalty=diversity_penalty,\n                                   length_penalty=length_penalty,\n                                   repetition_penalty=repetition_penalty,\n                                   no_repeat_ngram_size=2,\n                                   )\n\n        titles.append({\"Original\": abstract[\"Title\"],\n                       \"Titles\": [tokenizer.decode(title, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n                                  for title in title_ids]})\n\n    return titles","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_bert_score(original, candidates, mode=\"max\", verbose=False):\n    originals = [original] * len(candidates)\n    \n    p, r, f1 = score(candidates, originals, lang=\"en\")\n\n    if verbose:\n        print(f\"Original title: {original}\")\n        print()\n        for i, candidate in enumerate(candidates):\n            print(f\"{candidate}\")\n            print(f\"Precision: {p[i]:.3f}, Recall: {r[i]:.3f}, F1: {f1[i]:.3f}\")\n            print()\n        print()\n\n    if mode == \"best\":\n        return candidates[torch.argmax(f1).item()]\n    if mode == \"max\":\n        return torch.max(f1).item()\n    if mode == \"mean\":\n        return torch.mean(f1).item()\n    return p, r, f1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def quantitative_evaluation():\n    model_names = [\"2048_default\", \"2048_3e-4\", \"2048_3e-6\", \"2048_epochs\"]\n\n    tokenizer = load_tokenizer()\n\n    for model_name in model_names:\n        model = load_local_model(model_name)\n        model.resize_token_embeddings(len(tokenizer))\n        pairs = import_test_abstracts(model_name)\n\n        titles = get_titles_from_abstracts(model=model,\n                                           tokenizer=tokenizer,\n                                           abstracts=pairs,\n                                           max_length=25,\n                                           min_length=5)\n\n        f1 = []\n        for i, title_set in enumerate(titles):\n            if (i + 1) % 100 == 0:\n            print(f\"{i + 1}/{len(titles)}\")\n            \n            trimmed_titles = [title.split(\" <TITLE> \")[1].split(\"<|endoftext|>\")[0] for title in title_set[\"Titles\"]]\n\n            f1.append(calculate_bert_score(title_set[\"Original\"],\n                                           trimmed_titles,\n                                           mode=\"max\"))\n\n        print()\n        print(f1)\n        print(sum(f1) / len(f1))\n        \nquantitative_evaluation()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def qualitative_evaluation():\n    model_names = [\"2048_default\", \"2048_3e-4\", \"2048_3e-6\", \"2048_epochs\"]\n    pairs = get_abstracts_for_qual_eval()\n\n    for pair in pairs:\n        print(pair[\"Title\"])\n        print(pair[\"Abstract\"])\n        print()\n\n    tokenizer = load_tokenizer()\n\n    for model_name in model_names:\n        model = load_local_model(model_name)\n        model.resize_token_embeddings(len(tokenizer))\n        titles = get_titles_from_abstracts(model=model,\n                                           tokenizer=tokenizer,\n                                           abstracts=pairs,\n                                           max_length=25,\n                                           min_length=5)\n\n        for title_set in titles:\n            print(title_set[\"Original\"])\n            for title in title_set[\"Titles\"]:\n                print(title.split(\" <TITLE> \")[1].split(\"<|endoftext|>\")[0])\n            print()\n            \nqualitative_evaluation()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_titles_for_single_abstract(model_name, abstract)\n    model = load_local_model(model_name)\n    tokenizer = load_tokenizer()\n    model.resize_token_embeddings(len(tokenizer))\n    \n    abstracts = [{\"Title\": \"\", \"Abstract\": abstract}]\n    \n    titles = get_titles_from_abstracts(model, tokenizer, abstracts)\n    \n    for title in titles[0][\"Titles\"]:\n        print(title.split(\" <TITLE> \")[1].split(\"<|endoftext|>\")[0])\n    \n    return titles[0]\n    ","metadata":{},"execution_count":null,"outputs":[]}]}