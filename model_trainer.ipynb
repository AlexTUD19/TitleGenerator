{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U datasets\n!pip install transformers\n!pip install torch\n!pip install sklearn\n\nimport random\nimport torch\nimport datasets\nimport re\nimport json\nimport os\nfrom transformers import Trainer, TrainingArguments, GPT2DoubleHeadsModel,GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-10T15:44:44.956356Z","iopub.execute_input":"2021-08-10T15:44:44.956644Z","iopub.status.idle":"2021-08-10T15:45:19.972156Z","shell.execute_reply.started":"2021-08-10T15:44:44.956572Z","shell.execute_reply":"2021-08-10T15:45:19.971061Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:46:06.823263Z","iopub.execute_input":"2021-08-04T08:46:06.823589Z","iopub.status.idle":"2021-08-04T08:46:06.872163Z","shell.execute_reply.started":"2021-08-04T08:46:06.823561Z","shell.execute_reply":"2021-08-04T08:46:06.871118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model():\n    model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n    return model\n\nmodel = load_model()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:35:11.765272Z","iopub.execute_input":"2021-08-08T15:35:11.765592Z","iopub.status.idle":"2021-08-08T15:35:31.101834Z","shell.execute_reply.started":"2021-08-08T15:35:11.765561Z","shell.execute_reply":"2021-08-08T15:35:31.100791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_local_model():\n    model = GPT2DoubleHeadsModel.from_pretrained(\"../input/model3/\", local_files_only=True)\n    model.to(\"cuda\")\n    return model\n\nmodel = load_local_model()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-10T15:45:31.01859Z","iopub.execute_input":"2021-08-10T15:45:31.018979Z","iopub.status.idle":"2021-08-10T15:45:46.271704Z","shell.execute_reply.started":"2021-08-10T15:45:31.018947Z","shell.execute_reply":"2021-08-10T15:45:46.270839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_tokenizer():\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    special_tokens = {'bos_token':'<|startoftext|>','eos_token':'<|endoftext|>','pad_token':'<pad>','additional_special_tokens':['<TITLE>']} \n    tokenizer.add_special_tokens(special_tokens)\n    return tokenizer\n\ntokenizer = load_tokenizer()\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T15:45:52.811787Z","iopub.execute_input":"2021-08-10T15:45:52.812312Z","iopub.status.idle":"2021-08-10T15:45:57.823647Z","shell.execute_reply.started":"2021-08-10T15:45:52.812263Z","shell.execute_reply":"2021-08-10T15:45:57.822691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nimport os, glob\nimport pandas as pd\n\ndef edit_dataset():\n\n    with open(\"../input/new-title-abstract-pairs/new_title_abstract_pairs.csv\", newline=\"\") as csvfile: #input path of the dataset\n      reader = csv.reader(csvfile, delimiter=\",\")\n\n      lines = []\n      for row in reader:\n        lines.append(row[2] + \" <TITLE> \" + row[1] + ' <|endoftext|> ')\n\n      print(\"Max length:\", max([len(line) for line in lines]))\n      print(\"Average length:\", sum([len(line) for line in lines])/len(lines))  \n\n      lines[0] = \"inputs\"\n      with open(\"Advaced_combined.csv\", 'w', newline='') as newfile: # target file\n        writer = csv.writer(newfile, delimiter=',')\n        for line in lines:\n          if len(line) <= 1024:\n            writer.writerow([line])\n    \nedit_dataset()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:35:40.852318Z","iopub.execute_input":"2021-08-08T15:35:40.852648Z","iopub.status.idle":"2021-08-08T15:35:41.054332Z","shell.execute_reply.started":"2021-08-08T15:35:40.852615Z","shell.execute_reply":"2021-08-08T15:35:41.053502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dataset(tokenizer, path, size=-1):\n    dataset = datasets.load_dataset(\"csv\", data_files=path)\n    print(dataset)\n\n    def tokenize_input(examples):\n        inputs = tokenizer(examples[\"inputs\"], max_length=1024, padding=\"max_length\", truncation=True)\n\n        return inputs\n\n    train_dataset = dataset.map(tokenize_input, batched=True)\n\n    train, test = train_test_split(train_dataset[\"train\"],test_size=0.3)\n    \n    train_path = \"train_dataset.txt\"\n    test_path = \"test_dataset.txt\"\n    with open(train_path,\"w\") as outfile:\n      json.dump(train, outfile)\n    with open(test_path,\"w\") as outfile:\n      json.dump(test, outfile)  \n    \n\n    train_dataset = TextDataset(\n           tokenizer=tokenizer,\n           file_path=train_path,\n           block_size=128)\n \n    test_dataset = TextDataset(\n           tokenizer=tokenizer,\n           file_path=test_path,\n           block_size=128)\n \n    data_collator = DataCollatorForLanguageModeling(\n         tokenizer=tokenizer, mlm=False,\n     )\n    return train_dataset,test_dataset,data_collator,test\n\n\ntrain_dataset, eval_dataset,data_collator,test = load_dataset(tokenizer, path=\"./Advaced_combined.csv\", size=100)\nprint(train_dataset)\nprint(eval_dataset)\nprint(data_collator)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:35:44.340879Z","iopub.execute_input":"2021-08-08T15:35:44.341362Z","iopub.status.idle":"2021-08-08T15:36:42.612049Z","shell.execute_reply.started":"2021-08-08T15:35:44.341315Z","shell.execute_reply":"2021-08-08T15:36:42.610478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_dataset, eval_dataset, data_collator):\n    \n    \n    training_args = TrainingArguments(output_dir=\"./output\",\n                                      do_train=True,\n                                      evaluation_strategy=\"epoch\",\n                                      num_train_epochs=4,\n                                      per_device_train_batch_size=32,\n                                      per_device_eval_batch_size=64,\n                                      fp16=False,\n                                      learning_rate=3e-5,\n                                      gradient_accumulation_steps=8,\n\n                                      \n                                      report_to=None)\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator = data_collator\n    )\n\n   \n    \n\n    trainer.train()\n\n\ntrain_model(model, train_dataset, eval_dataset, data_collator)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T15:37:00.532144Z","iopub.execute_input":"2021-08-08T15:37:00.532633Z","iopub.status.idle":"2021-08-08T17:12:25.365981Z","shell.execute_reply.started":"2021-08-08T15:37:00.53259Z","shell.execute_reply":"2021-08-08T17:12:25.364883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"./\")","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:25:18.093656Z","iopub.execute_input":"2021-08-08T17:25:18.094017Z","iopub.status.idle":"2021-08-08T17:25:19.306454Z","shell.execute_reply.started":"2021-08-08T17:25:18.093983Z","shell.execute_reply":"2021-08-08T17:25:19.305612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_bertScore(model, train_dataset, eval_dataset, data_collator):\n    model.eval()\n    metric = datasets.load_metric(\"bertscore\")\n    gpu_usage()\n    torch.cuda.empty_cache()\n    gpu_usage()\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        print(\"predicitons:\")\n        print(predictions)\n        print(\"labels: \")\n        print(labels)\n        return metric.compute(predictions=predictions, references=labels)\n\n    training_args = TrainingArguments(\"test_trainer\")\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    trainer.evaluate()\ncompute_bertScore(model, train_dataset, eval_dataset, data_collator)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}